{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j GraphRAG test\n",
    "\n",
    "https://github.com/neo4j-product-examples/graphrag-python-examples/blob/main/end-to-end-lupus.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load neo4j credentials (and openai api key in background).\n",
    "load_dotenv('.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "\n",
    "#uncomment this line if you aren't using a .env file\n",
    "# os.environ['OPENAI_API_KEY'] = 'copy_paste_the_openai_key_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import neo4j\n",
    "from neo4j_graphrag.embeddings.openai import OpenAIEmbeddings\n",
    "from neo4j_graphrag.llm import AzureOpenAILLM\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# from neo4j_graphrag.llm import OpenAILLM\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "# ex_llm=OpenAILLM(\n",
    "#     model_name=os.getenv(\"GRAPHRAG_LLM_MODEL\"),\n",
    "#     model_params={\n",
    "#         \"response_format\": {\"type\": \"json_object\"}, # use json_object formatting for best results\n",
    "#         \"temperature\": 0 # turning temperature down for more deterministic results\n",
    "#     }\n",
    "# )\n",
    "\n",
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "ex_llm = AzureOpenAILLM(\n",
    "    model_name=os.getenv(\"GRAPHRAG_LLM_MODEL\"),\n",
    "    azure_endpoint=os.getenv(\"GRAPHRAG_API_BASE\"),  # update with your endpoint\n",
    "    api_version=os.getenv(\"GRAPHRAG_API_VERSION\"),  # update appropriate version\n",
    "    api_key=os.getenv(\"GRAPHRAG_API_KEY\"),  # api_key is optional and can also be set with OPENAI_API_KEY env var\n",
    ")\n",
    "\n",
    "#create text embedder\n",
    "embedder = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define node labels\n",
    "basic_node_labels = [\"Object\", \"Entity\", \"Group\", \"Person\", \"Organization\", \"Place\"]\n",
    "\n",
    "academic_node_labels = [\"ArticleOrPaper\", \"PublicationOrJournal\"]\n",
    "\n",
    "medical_node_labels = [\"Anatomy\", \"BiologicalProcess\", \"Cell\", \"CellularComponent\", \n",
    "                       \"CellType\", \"Condition\", \"Disease\", \"Drug\",\n",
    "                       \"EffectOrPhenotype\", \"Exposure\", \"GeneOrProtein\", \"Molecule\",\n",
    "                       \"MolecularFunction\", \"Pathway\"]\n",
    "\n",
    "node_labels = basic_node_labels + academic_node_labels + medical_node_labels\n",
    "\n",
    "# define relationship types\n",
    "rel_types = [\"ACTIVATES\", \"AFFECTS\", \"ASSESSES\", \"ASSOCIATED_WITH\", \"AUTHORED\",\n",
    "    \"BIOMARKER_FOR\", \"CAUSES\", \"CITES\", \"CONTRIBUTES_TO\", \"DESCRIBES\", \"EXPRESSES\",\n",
    "    \"HAS_REACTION\", \"HAS_SYMPTOM\", \"INCLUDES\", \"INTERACTS_WITH\", \"PRESCRIBED\",\n",
    "    \"PRODUCES\", \"RECEIVED\", \"RESULTS_IN\", \"TREATS\", \"USED_FOR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''\n",
    "You are a medical researcher tasks with extracting information from papers \n",
    "and structuring it in a property graph to inform further medical and research Q&A.\n",
    "\n",
    "Extract the entities (nodes) and specify their type from the following Input text.\n",
    "Also extract the relationships between these nodes. the relationship direction goes from the start node to the end node. \n",
    "\n",
    "\n",
    "Return result as JSON using the following format:\n",
    "{{\"nodes\": [ {{\"id\": \"0\", \"label\": \"the type of entity\", \"properties\": {{\"name\": \"name of entity\" }} }}],\n",
    "  \"relationships\": [{{\"type\": \"TYPE_OF_RELATIONSHIP\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {{\"details\": \"Description of the relationship\"}} }}] }}\n",
    "\n",
    "- Use only the information from the Input text.  Do not add any additional information.  \n",
    "- If the input text is empty, return empty Json. \n",
    "- Make sure to create as many nodes and relationships as needed to offer rich medical context for further research.\n",
    "- An AI knowledge assistant must be able to read this graph and immediately understand the context to inform detailed research questions. \n",
    "- Multiple documents will be ingested from different sources and we are using this property graph to connect information, so make sure entity types are fairly general. \n",
    "\n",
    "Use only fhe following nodes and relationships (if provided):\n",
    "{schema}\n",
    "\n",
    "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
    "Do respect the source and target node types for relationship and\n",
    "the relationship direction.\n",
    "\n",
    "Do not return any additional information other than the JSON in it.\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Input text:\n",
    "\n",
    "{text}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/bingranyou/Documents/GitHub_Mac_mini/DeepTutor/input_files/RankRAG- Unifying Context Ranking with  Retrieval-Augmented Generation in LLMs.pdf\n",
      "Skipping /Users/bingranyou/Documents/GitHub_Mac_mini/DeepTutor/input_files/RankRAG- Unifying Context Ranking with  Retrieval-Augmented Generation in LLMs.pdf; already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bingran_you/.local/lib/python3.12/site-packages/neo4j/_sync/work/result.py:620: UserWarning: Expected a result with a single record, but found multiple.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from neo4j_graphrag.experimental.components.text_splitters.fixed_size_splitter import FixedSizeSplitter\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "\n",
    "kg_builder_pdf = SimpleKGPipeline(\n",
    "    llm=ex_llm,\n",
    "    driver=driver,\n",
    "    text_splitter=FixedSizeSplitter(chunk_size=500, chunk_overlap=100),\n",
    "    embedder=embedder,\n",
    "    entities=node_labels,\n",
    "    relations=rel_types,\n",
    "    prompt_template=prompt_template,\n",
    "    from_pdf=True\n",
    ")\n",
    "\n",
    "pdf_file_paths = ['/Users/bingranyou/Documents/GitHub_Mac_mini/DeepTutor/input_files/RankRAG- Unifying Context Ranking with  Retrieval-Augmented Generation in LLMs.pdf']\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def document_exists(driver, file_path):\n",
    "    query = \"\"\"\n",
    "    MATCH (d:Document {path: $file_path})\n",
    "    RETURN d\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, file_path=file_path)\n",
    "        return result.single() is not None\n",
    "\n",
    "for path in pdf_file_paths:\n",
    "    print(f\"Processing: {path}\")\n",
    "    if document_exists(driver, path):\n",
    "        print(f\"Skipping {path}; already processed.\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Processing : {path}\")\n",
    "        pdf_result = await kg_builder_pdf.run_async(file_path=path)\n",
    "        print(f\"Result: {pdf_result}\")\n",
    "    pdf_result = await kg_builder_pdf.run_async(file_path=path)\n",
    "    print(f\"Result: {pdf_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph Retrieval\n",
    "We will leverage Neo4j's vector search capabilities here. To do this, we need to begin by creating a vector index on the text chunks from the PDFs, which are stored on Chunk nodes in our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "\n",
    "create_vector_index(driver, name=\"text_embeddings\", label=\"Chunk\",\n",
    "                    embedding_property=\"embedding\", dimensions=1536, similarity_fn=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "\n",
    "vector_retriever = VectorRetriever(\n",
    "    driver,\n",
    "    index_name=\"text_embeddings\",\n",
    "    embedder=embedder,\n",
    "    return_properties=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====\n",
      "{\n",
      "    \"node\": {\n",
      "        \"text\": \"n the evaluations of ranking associated with the RAG\\ntasks, even surpassing the LLMs fine-tuned with 10\\u00d7more ranking data. We attribute this success\\nto the transferable design of RankRAG training.\\n\\u2022We extensively compare the proposed RankRAG method with several strong baselines, including\\nthe open-sourced ChatQA-1.5. On nine general-domain and five biomedical knowledge-intensive\\nbenchmarks for RAG, Llama3-RankRAG-8B and Llama3-RankRAG-70B outperforms Llama3-\\nChatQA-1.5-8B and Llama3-ChatQA-1.5-7\"\n",
      "    },\n",
      "    \"nodeLabels\": [\n",
      "        \"__KGBuilder__\",\n",
      "        \"Chunk\"\n",
      "    ],\n",
      "    \"elementId\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1585\",\n",
      "    \"id\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1585\",\n",
      "    \"score\": 0.926300048828125\n",
      "}\n",
      "====\n",
      "{\n",
      "    \"node\": {\n",
      "        \"text\": \"n the evaluations of ranking associated with the RAG\\ntasks, even surpassing the LLMs fine-tuned with 10\\u00d7more ranking data. We attribute this success\\nto the transferable design of RankRAG training.\\n\\u2022We extensively compare the proposed RankRAG method with several strong baselines, including\\nthe open-sourced ChatQA-1.5. On nine general-domain and five biomedical knowledge-intensive\\nbenchmarks for RAG, Llama3-RankRAG-8B and Llama3-RankRAG-70B outperforms Llama3-\\nChatQA-1.5-8B and Llama3-ChatQA-1.5-7\"\n",
      "    },\n",
      "    \"nodeLabels\": [\n",
      "        \"__KGBuilder__\",\n",
      "        \"Chunk\"\n",
      "    ],\n",
      "    \"elementId\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1947\",\n",
      "    \"id\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1947\",\n",
      "    \"score\": 0.9261627197265625\n",
      "}\n",
      "====\n",
      "{\n",
      "    \"node\": {\n",
      "        \"text\": \", Llama3-RankRAG-8B and Llama3-RankRAG-70B outperforms Llama3-\\nChatQA-1.5-8B and Llama3-ChatQA-1.5-70B by a margin, respectively.\\nIn the remainder of the paper, we discuss related work in \\u00a7 2. We introduce problem setup in \\u00a7 3 and\\nRankRAG method in \\u00a7 4. We present the experimental setup in \\u00a7 5, and conclude the paper in \\u00a7 6.\\n2 Related Work\\nRetrieval-augumented generation (RAG) has been established for knowledge-intensive NLP\\ntasks (Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023;\"\n",
      "    },\n",
      "    \"nodeLabels\": [\n",
      "        \"__KGBuilder__\",\n",
      "        \"Chunk\"\n",
      "    ],\n",
      "    \"elementId\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1586\",\n",
      "    \"id\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1586\",\n",
      "    \"score\": 0.9196319580078125\n",
      "}\n",
      "====\n",
      "{\n",
      "    \"node\": {\n",
      "        \"text\": \"rformance on\\nmany RAG benchmarks. RankRAG 8B is also competitive when compared with baseline models\\nwith much more parameters. For example, it significantly outperforms InstructRetro ( 5\\u00d7parameters),\\nRA-DIT 65B ( 8\\u00d7paramters), and even outperforms Llama3-instruct 70B ( 8\\u00d7parameters) on NQ\\nand TriviaQA tasks. With more parameters, RankRAG 70B outperforms the strong ChatQA-1.5 70B\\nmodel, and largely outperforms previous RAG baselines with InstructGPT as the underlying LLM.\\nRankRAG demonstrates lar\"\n",
      "    },\n",
      "    \"nodeLabels\": [\n",
      "        \"__KGBuilder__\",\n",
      "        \"Chunk\"\n",
      "    ],\n",
      "    \"elementId\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1629\",\n",
      "    \"id\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1629\",\n",
      "    \"score\": 0.9196014404296875\n",
      "}\n",
      "====\n",
      "{\n",
      "    \"node\": {\n",
      "        \"text\": \", Llama3-RankRAG-8B and Llama3-RankRAG-70B outperforms Llama3-\\nChatQA-1.5-8B and Llama3-ChatQA-1.5-70B by a margin, respectively.\\nIn the remainder of the paper, we discuss related work in \\u00a7 2. We introduce problem setup in \\u00a7 3 and\\nRankRAG method in \\u00a7 4. We present the experimental setup in \\u00a7 5, and conclude the paper in \\u00a7 6.\\n2 Related Work\\nRetrieval-augumented generation (RAG) has been established for knowledge-intensive NLP\\ntasks (Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023;\"\n",
      "    },\n",
      "    \"nodeLabels\": [\n",
      "        \"__KGBuilder__\",\n",
      "        \"Chunk\"\n",
      "    ],\n",
      "    \"elementId\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1953\",\n",
      "    \"id\": \"4:cbfd137b-822c-461a-a486-5bb936f239f6:1953\",\n",
      "    \"score\": 0.9191741943359375\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "vector_res = vector_retriever.get_search_results(query_text = \"What is RankRAG?\", \n",
    "                                                 top_k=5)\n",
    "for i in vector_res.records: print(\"====\\n\" + json.dumps(i.data(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorCypherRetriever\n",
    "\n",
    "vc_retriever = VectorCypherRetriever(\n",
    "    driver,\n",
    "    index_name=\"text_embeddings\",\n",
    "    embedder=embedder,\n",
    "    retrieval_query=\"\"\"\n",
    "//1) Go out 2-3 hops in the entity graph and get relationships\n",
    "WITH node AS chunk\n",
    "MATCH (chunk)<-[:FROM_CHUNK]-()-[relList:!FROM_CHUNK]-{1,2}()\n",
    "UNWIND relList AS rel\n",
    "\n",
    "//2) collect relationships and text chunks\n",
    "WITH collect(DISTINCT chunk) AS chunks, \n",
    "  collect(DISTINCT rel) AS rels\n",
    "\n",
    "//3) format and return context\n",
    "RETURN '=== text ===\\n' + apoc.text.join([c in chunks | c.text], '\\n---\\n') + '\\n\\n=== kg_rels ===\\n' +\n",
    "  apoc.text.join([r in rels | startNode(r).name + ' - ' + type(r) + '(' + coalesce(r.details, '') + ')' +  ' -> ' + endNode(r).name ], '\\n---\\n') AS info\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Text Chunk Context:\n",
      "=== text ===\n",
      "6]. Ontheotherhand,\n",
      "implementing a multiplexed light-matter interface with\n",
      "these systems is technically challenging. Towards over-\n",
      "coming this problem, a few multiplexing schemes have\n",
      "already been proposed for ion and atom-based quan-\n",
      "tum processors [27–30]. The only reported experimental\n",
      "work, we are aware of, is the demonstration of multiplex-\n",
      "ing using a static three-ion chain [15]. In view of the re-\n",
      "cent advances of the quantum CCD architecture [31–33],\n",
      "a complementary approach to multiplex\n",
      "---\n",
      "6]. Ontheotherhand,\n",
      "implementing a multiplexed light-matter interface with\n",
      "these systems is technically challenging. Towards over-\n",
      "coming this problem, a few multiplexing schemes have\n",
      "already been proposed for ion and atom-based quan-\n",
      "tum processors [27–30]. The only reported experimental\n",
      "work, we are aware of, is the demonstration of multiplex-\n",
      "ing using a static three-ion chain [15]. In view of the re-\n",
      "cent advances of the quantum CCD architecture [31–33],\n",
      "a complementary approach to multiplex\n",
      "---\n",
      "the re-\n",
      "cent advances of the quantum CCD architecture [31–33],\n",
      "a complementary approach to multiplexing is the process\n",
      "of ion-transport through a specific spatial location with\n",
      "maximized photon coupling efficiency.\n",
      "In this work, we demonstrate a temporal multiplexing\n",
      "schemebasedonthetransportofanion-chainforimprov-\n",
      "ing the rate of ion-photon entanglement over long dis-\n",
      "tances. In our experiments, we generate on-demand sin-\n",
      "gle photons by shuttling a nine-ion chain across the focus\n",
      "of a single-io\n",
      "# KG Context From Relationships:\n",
      "\n",
      "\n",
      "=== kg_rels ===\n",
      "RankRAG - USES() -> RAG framework\n",
      "---\n",
      "RankRAG - DESCRIBES() -> answer generation\n",
      "---\n",
      "dual capability - CONTRIBUTES_TO() -> answer generation\n",
      "---\n",
      "RankRAG - DESCRIBES() -> context ranking\n",
      "---\n",
      "dual capability - CONTRIBUTES_TO() -> context ranking\n",
      "---\n",
      "RankRAG - AUTHORED() -> Lin et al.\n",
      "---\n",
      "RankRAG - AUTHORED() -> Liu et al.\n",
      "---\n",
      "RankRAG - AUTHORED() -> OpenAI\n",
      "---\n",
      "quantum CCD architecture - INCLUDES(Quantum CCD architecture includes ion-transport) -> ion-transport\n",
      "---\n",
      "ion-transport - AFFECTS(Ion-transport affects photon coupling efficiency) -> photon coupling efficiency\n",
      "---\n",
      "quantum CCD architecture - ASSOCIATED_WITH(quantum CCD architecture is associated with ion-transport) -> ion-transport\n",
      "---\n",
      "nine-ion chain - INTERACTS_WITH(Nine-ion chain interacts with photon coupling efficiency) -> photon coupling efficiency\n",
      "---\n",
      "nine-ion chain - RESULTS_IN(Nine-ion chain results in generation of single photons) -> single photons\n",
      "---\n",
      "nine-ion chain - INTERACTS_WITH(nine-ion chain interacts with single-ion addressing beam) -> single-ion addressing beam\n",
      "---\n",
      "sideband-cooled motional spectrum - INCLUDES(The motional spectrum includes the nine-ion chain.) -> nine-ion chain\n",
      "---\n",
      "temporal multiplexing scheme - INCLUDES(temporal multiplexing scheme includes a nine-ion chain) -> nine-ion chain\n",
      "---\n",
      "nine-ion chain - PRODUCES(nine-ion chain generates single photons) -> single photons\n",
      "---\n",
      "nine-ion chain - HAS_REACTION(nine-ion chain has a reaction with single-ion focus) -> single-ion focus\n",
      "---\n",
      "temporal multiplexing scheme - DESCRIBES(temporal multiplexing scheme describes the process of ion-photon entanglement) -> ion-photon entanglement\n",
      "---\n",
      "temporal multiplexing scheme - PRODUCES(Temporal multiplexing scheme produces ion-photon entanglement) -> ion-photon entanglement\n",
      "---\n",
      "temporal multiplexing scheme - USED_FOR(Temporal multiplexing scheme is used for generating single photons) -> single photons\n",
      "---\n",
      "photomultiplier tube - DETECTS(photomultiplier tube detects single photons) -> single photons\n",
      "---\n",
      "photon trains - VERIFIES(photon trains verify the nature of single photons) -> single photons\n",
      "---\n",
      "motional excitation - AFFECTS(motional excitation affects single photons) -> single photons\n",
      "---\n",
      "second-order time correlation - ASSESSES(second-order time correlation assesses single photon nature) -> single photons\n",
      "---\n",
      "beam splitter - INTERACTS_WITH(beam splitter interacts with single photons) -> single photons\n",
      "---\n",
      "bichromatic cavity-mediated Raman transition (BCMRT) - PRODUCES(Single photons are generated via BCMRT.) -> single photons\n",
      "---\n",
      "beam - PRODUCES(beam produces single photons) -> single photons\n",
      "---\n",
      "resonant excitation - PRODUCES(Resonant excitation extracts single photons) -> single photons\n",
      "---\n",
      "temporal multiplexing scheme - PRODUCES(temporal multiplexing scheme produces single photons) -> single photons\n",
      "---\n",
      "Raman laser beam - INTERACTS_WITH(The Raman laser beam drives the BCMRT.) -> bichromatic cavity-mediated Raman transition (BCMRT)\n",
      "---\n",
      "bichromatic cavity-mediated Raman transition (BCMRT) - ACTIVATES(BCMRT is driven via the 393-nm Raman laser beam) -> 393-nm Raman laser beam\n",
      "---\n",
      "beam - INCLUDES(beam includes beam waist) -> beam waist\n",
      "---\n",
      "addressing beam - HAS_REACTION(Addressing beam allows for resonant excitation) -> resonant excitation\n",
      "---\n",
      "single-ion addressing beam - RESULTS_IN(single-ion addressing beam leads to entanglement generation) -> entanglement generation\n",
      "---\n",
      "nth number state - RESULTS_IN(Effectiveness of approximation verified by probing the motional spectrum.) -> sideband-cooled motional spectrum\n"
     ]
    }
   ],
   "source": [
    "vc_res = vc_retriever.get_search_results(query_text = \"What is multiplexing\", top_k=3)\n",
    "\n",
    "# print output\n",
    "kg_rel_pos = vc_res.records[0]['info'].find('\\n\\n=== kg_rels ===\\n')\n",
    "print(\"# Text Chunk Context:\")\n",
    "print(vc_res.records[0]['info'][:kg_rel_pos])\n",
    "print(\"# KG Context From Relationships:\")\n",
    "print(vc_res.records[0]['info'][kg_rel_pos:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import neo4j\n",
    "from neo4j_graphrag.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "ex_llm = AzureOpenAILLM(\n",
    "    model_name=os.getenv(\"GRAPHRAG_LLM_MODEL\"),\n",
    "    azure_endpoint=os.getenv(\"GRAPHRAG_API_BASE\"),  # update with your endpoint\n",
    "    api_version=os.getenv(\"GRAPHRAG_API_VERSION\"),  # update appropriate version\n",
    "    api_key=os.getenv(\"GRAPHRAG_API_KEY\"),  # api_key is optional and can also be set with OPENAI_API_KEY env var\n",
    ")\n",
    "\n",
    "#create text embedder\n",
    "embedder = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.llm import AzureOpenAILLM\n",
    "from neo4j_graphrag.generation import RagTemplate\n",
    "from neo4j_graphrag.generation.graphrag import GraphRAG\n",
    "\n",
    "llm = AzureOpenAILLM(\n",
    "    model_name=os.getenv(\"GRAPHRAG_LLM_MODEL\"),\n",
    "    azure_endpoint=os.getenv(\"GRAPHRAG_API_BASE\"),  # update with your endpoint\n",
    "    api_version=os.getenv(\"GRAPHRAG_API_VERSION\"),  # update appropriate version\n",
    "    api_key=os.getenv(\"GRAPHRAG_API_KEY\"),  # api_key is optional and can also be set with OPENAI_API_KEY env var\n",
    ")\n",
    "\n",
    "rag_template = RagTemplate(template='''Answer the Question using the following Context. Only respond with information mentioned in the Context. Do not inject any speculative information not mentioned. \n",
    "\n",
    "# Question:\n",
    "{query_text}\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "# Answer:\n",
    "''', expected_inputs=['query_text', 'context'])\n",
    "\n",
    "v_rag  = GraphRAG(llm=llm, retriever=vector_retriever, prompt_template=rag_template)\n",
    "vc_rag = GraphRAG(llm=llm, retriever=vc_retriever, prompt_template=rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Response: \n",
      "Multiplexing is the process of ion-transport through a specific spatial location with maximized photon coupling efficiency.\n",
      "\n",
      "===========================\n",
      "\n",
      "Vector + Cypher Response: \n",
      "Multiplexing is a scheme to implement a light-matter interface, technically challenging with certain systems, and in quantum processors, it involves schemes for ion and atom-based systems. A complementary approach to multiplexing mentioned is ion-transport through a specific spatial location with maximized photon coupling efficiency.\n"
     ]
    }
   ],
   "source": [
    "q = \"What is multiplexing.\"\n",
    "print(f\"Vector Response: \\n{v_rag.search(q, retriever_config={'top_k':5}).answer}\")\n",
    "print(\"\\n===========================\\n\")\n",
    "print(f\"Vector + Cypher Response: \\n{vc_rag.search(q, retriever_config={'top_k':5}).answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Response: \n",
      "A quantum network involves the entanglement between matter qubits and photonic qubits. This entanglement can be distributed over long distances, such as a 101-km-long fiber channel. The network can enhance processing capabilities, for example, by using multiple cotrapped matter qubits at a node.\n",
      "\n",
      "===========================\n",
      "\n",
      "Vector + Cypher Response: \n",
      "A key requirement for long-distance quantum networking is the ability to entangle a matter qubit with a photon and to distribute that photon over many tens of kilometers.\n"
     ]
    }
   ],
   "source": [
    "q = \"What is quatnum network\"\n",
    "\n",
    "v_rag_result = v_rag.search(q, retriever_config={'top_k': 5}, return_context=True)\n",
    "vc_rag_result = vc_rag.search(q, retriever_config={'top_k': 5}, return_context=True)\n",
    "\n",
    "print(f\"Vector Response: \\n{v_rag_result.answer}\")\n",
    "print(\"\\n===========================\\n\")\n",
    "print(f\"Vector + Cypher Response: \\n{vc_rag_result.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"text\": \"ned for the most advanced forms of a quantum\\nnetwork [ 2].\\nIn this paper, we present two main results. First, entan-\\nglement between a matter qubit and a photonic qubit is\\nachieved over a spooled 101-km-long \\ufb01ber channel: twice\\nthe distance of previous works (see, e.g., Refs. [ 15\\u201318])\\nand requiring a matter-qubit coherence time on the order of\\nthe photon travel time (494 \\u00b5s) to achieve. Second, using\\nthree cotrapped matter qubits in the node, we demonstrate\\na multimoding enhancement for the rat\"\n",
      "}\n",
      "{\n",
      " \"text\": \"ned for the most advanced forms of a quantum\\nnetwork [ 2].\\nIn this paper, we present two main results. First, entan-\\nglement between a matter qubit and a photonic qubit is\\nachieved over a spooled 101-km-long \\ufb01ber channel: twice\\nthe distance of previous works (see, e.g., Refs. [ 15\\u201318])\\nand requiring a matter-qubit coherence time on the order of\\nthe photon travel time (494 \\u00b5s) to achieve. Second, using\\nthree cotrapped matter qubits in the node, we demonstrate\\na multimoding enhancement for the rat\"\n",
      "}\n",
      "{\n",
      " \"text\": \"ned for the most advanced forms of a quantum\\nnetwork [ 2].\\nIn this paper, we present two main results. First, entan-\\nglement between a matter qubit and a photonic qubit is\\nachieved over a spooled 101-km-long \\ufb01ber channel: twice\\nthe distance of previous works (see, e.g., Refs. [ 15\\u201318])\\nand requiring a matter-qubit coherence time on the order of\\nthe photon travel time (494 \\u00b5s) to achieve. Second, using\\nthree cotrapped matter qubits in the node, we demonstrate\\na multimoding enhancement for the rat\"\n",
      "}\n",
      "{\n",
      " \"text\": \"lementary net-\\nworks consisting of two [ 3\\u201311] and three [ 12] remote\\nmatter qubits, distributed over distances up to 1.5 km [ 13].\\nRecently, two atoms 400 m apart have been entangled over\\na spooled 33 km-long \\ufb01ber channel [ 14].\\nA key requirement for long-distance quantum network-\\ning is the ability to entangle a matter qubit with a photon\\nand to distribute that photon over many tens of kilometers.\\nThat ability has been demonstrated using a range of di\\ufb00er-\\nent systems including trapped ions [ 1\"\n",
      "}\n",
      "{\n",
      " \"text\": \"lementary net-\\nworks consisting of two [ 3\\u201311] and three [ 12] remote\\nmatter qubits, distributed over distances up to 1.5 km [ 13].\\nRecently, two atoms 400 m apart have been entangled over\\na spooled 33 km-long \\ufb01ber channel [ 14].\\nA key requirement for long-distance quantum network-\\ning is the ability to entangle a matter qubit with a photon\\nand to distribute that photon over many tens of kilometers.\\nThat ability has been demonstrated using a range of di\\ufb00er-\\nent systems including trapped ions [ 1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i in v_rag_result.retriever_result.items: print(json.dumps(eval(i.content), indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_ls = vc_rag_result.retriever_result.items[0].content.split('\\\\n---\\\\n')\n",
    "for i in vc_ls:\n",
    "    if \"biomarker\" in i: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_ls = vc_rag_result.retriever_result.items[0].content.split('\\\\n---\\\\n')\n",
    "for i in vc_ls:\n",
    "    if \"treat\" in i: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Response: \n",
      "RankRAG is a method designed for retrieval-augmented generation (RAG) tasks in NLP. It has been evaluated against several strong baselines and demonstrated superior performance, even surpassing models fine-tuned with significantly more ranking data. This success is attributed to the transferable design of RankRAG training. RankRAG models, such as Llama3-RankRAG-8B and Llama3-RankRAG-70B, outperform other models like Llama3-ChatQA-1.5-8B and Llama3-ChatQA-1.5-70B across various benchmarks.\n",
      "\n",
      "===========================\n",
      "\n",
      "Vector + Cypher Response: \n",
      "RankRAG is a method used for answer generation and context ranking in the evaluations of ranking associated with the RAG (Retrieval-Augmented Generation) tasks.\n"
     ]
    }
   ],
   "source": [
    "q = \"What is RankRAG?\"\n",
    "print(f\"Vector Response: \\n{v_rag.search(q, retriever_config={'top_k': 5}).answer}\")\n",
    "print(\"\\n===========================\\n\")\n",
    "print(f\"Vector + Cypher Response: \\n{vc_rag.search(q, retriever_config={'top_k': 5}).answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
